ğŸ“š RAG-Based PDF Question Answering Chatbot

A live, deployed Retrieval-Augmented Generation (RAG) application that allows users to ask questions from a PDF document and receive context-aware answers using LLMs.

ğŸš€ Live Demo

ğŸ”— Live App:
https://rag-pdf-chatbot-wdxgjqd6imdwgreusibgdj.streamlit.app/

ğŸ’» GitHub Repository:
https://github.com/venkatraja1002-sudo/rag-pdf-chatbot

ğŸ§  Project Overview

This project implements a complete RAG pipeline:
Load PDF document
Split into text chunks
Generate embeddings
Store embeddings in FAISS vector database
Retrieve relevant chunks
Send context + question to Groq LLaMA 3.1 model
Display answer via Streamlit UI

ğŸ— Architecture
User Question
      â†“
Streamlit UI
      â†“
Retriever (FAISS)
      â†“
Relevant Chunks
      â†“
Groq LLM (LLaMA 3.1)
      â†“
Final Answer


ğŸ›  Tech Stack
Python
Streamlit
LangChain
FAISS (Vector Database)
Sentence Transformers
Groq LLaMA 3.1
Git & GitHub
Streamlit Community Cloud (Deployment)

ğŸ“‚ Project Structure
rag-pdf-chatbot/
â”‚
â”œâ”€â”€ app.py                 # Streamlit frontend
â”œâ”€â”€ rag_pipeline.py        # RAG backend logic
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ .gitignore             # Ignore local files
â”‚
â””â”€â”€ data/
    â””â”€â”€ sample.pdf         # Document used for Q&A

âš™ï¸ Installation (Run Locally)
1ï¸âƒ£ Clone Repository
git clone https://github.com/venkatraja1002-sudo/rag-pdf-chatbot.git
cd rag-pdf-chatbot
2ï¸âƒ£ Create Virtual Environment
python -m venv .venv
.venv\Scripts\activate
3ï¸âƒ£ Install Requirements
pip install -r requirements.txt
4ï¸âƒ£ Add GROQ API Key
Create a .env file:
GROQ_API_KEY=your_api_key_here
5ï¸âƒ£ Run App
streamlit run app.py

ğŸŒ Deployment

This app is deployed using Streamlit Community Cloud.

